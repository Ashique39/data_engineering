{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be53e1fc-3405-4727-a664-faf364037095",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa97db07-03cd-44a0-be33-95180808f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col,to_date,hour,asc,desc,isnan,when,count\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a7a1e-9235-47be-9c8f-13fe9b8370e5",
   "metadata": {},
   "source": [
    "### Initiating Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1e03fde-921b-42e6-873c-66c4e5c707c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:44:10 WARN Utils: Your hostname, Aayushi resolves to a loopback address: 127.0.1.1; using 172.18.33.221 instead (on interface eth0)\n",
      "23/09/07 19:44:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hadoopuser/spark-3.0.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/09/07 19:44:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/07 19:44:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"bds\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b54be",
   "metadata": {},
   "source": [
    "#### Verifying the source files in the HDFS folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5072d48b-ff4b-4193-8fbb-61853344a399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 hadoopuser supergroup      10724 2023-09-01 21:09 /BDS_G106/taxi+_zone_lookup.csv\n",
      "-rw-r--r--   1 hadoopuser supergroup   45371782 2023-09-01 21:10 /BDS_G106/yellow_tripdata_2020-06.xlsx\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /BDS_G106/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c3e36-8c49-4da8-9c73-156d763c0ce2",
   "metadata": {},
   "source": [
    "## 1. Read the data in yellow_tripdata_2020-06.csv file into a dataframe created in spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16d210",
   "metadata": {},
   "source": [
    "##### Reading the yellow_tripdata_2020-06.xlsx file from hdfs using pandas read_excel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff58430-5ab2-4d1a-b82a-7a4fa882c58a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 19:44:14,508 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Reading the yellow_tripdata_2020-06.xlsx file from hdfs using pandas read_excel function\n",
    "\n",
    "df_yellow_tripdata = pd.read_excel(\"hdfs://localhost:9000/BDS_G106/yellow_tripdata_2020-06.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbf2de4-278b-4f86-9f5d-be8311c71329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:46:18 WARN TaskSetManager: Stage 0 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|2020-06-01 00:31:23 |2020-06-01 00:49:58  |1.0            |3.6          |140         |68          |1.0         |15.5       |3.0  |0.5    |4.0       |0.0         |0.3                  |23.3        |\n",
      "|2020-06-01 00:42:50 |2020-06-01 01:04:33  |1.0            |5.6          |79          |226         |1.0         |19.5       |3.0  |0.5    |2.0       |0.0         |0.3                  |25.3        |\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_tripdata.iteritems = df_yellow_tripdata.items\n",
    "df_spark_yellow_tripdata = spark.createDataFrame(df_yellow_tripdata)\n",
    "df_spark_yellow_tripdata.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4304167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_tripdata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523aee5-75d6-4c34-a7e1-8b4ef6ae0774",
   "metadata": {},
   "source": [
    "##### Extracting 3 new columns from tpep_pickup_datetime and tpep_dropoff_datetime:\n",
    "1. pick_date\n",
    "2. pick_date_hour\n",
    "3. pick_hour\n",
    "4. drop_date\n",
    "5. drop_date_hour\n",
    "6. drop_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c630145b-cb98-4e28-bf6b-1d1fd5eac6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 2 new columns pick_date and drop_date by converting the columns tpep_pickup_datetime and tpep_dropoff_datetime into timestamp format\n",
    "\n",
    "df_spark_taxi_trip = df_spark_yellow_tripdata.withColumn('pick_date',F.to_timestamp('tpep_pickup_datetime','yyyy-MM-dd HH:mm:ss')).withColumn('drop_date',F.to_timestamp('tpep_dropoff_datetime','yyyy-MM-dd HH:mm:ss'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea356063-711a-4671-a54a-8cd5835d50ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 4 new columns pick_date_hour, pick_hour, drop_date_hour and drop_hour. \n",
    "# pick_date_hour and drop_date_hour contains the date and the hour removing the minutes and seconds\n",
    "# pick_hour and drop_hour contains the hour part only\n",
    "\n",
    "df_spark_taxi_trip_data = df_spark_taxi_trip.withColumn('pick_date_hour', F.date_trunc('hour', F.to_timestamp('pick_date', \"yyyy-MM-dd HH:mm:ss 'UTC'\"))).withColumn('pick_hour',hour(col('pick_date_hour'))).withColumn('drop_date_hour', F.date_trunc('hour', F.to_timestamp('drop_date', \"yyyy-MM-dd HH:mm:ss 'UTC'\"))).withColumn('drop_hour',hour(col('drop_date_hour'))).select('*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "560688c3-9a7b-4540-a1e5-f03f176082c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+-------------------+-------------------+---------+-------------------+---------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|pick_date          |drop_date          |pick_date_hour     |pick_hour|drop_date_hour     |drop_hour|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+-------------------+-------------------+---------+-------------------+---------+\n",
      "|2020-06-01 00:31:23 |2020-06-01 00:49:58  |1.0            |3.6          |140         |68          |1.0         |15.5       |3.0  |0.5    |4.0       |0.0         |0.3                  |23.3        |2020-06-01 00:31:23|2020-06-01 00:49:58|2020-06-01 00:00:00|0        |2020-06-01 00:00:00|0        |\n",
      "|2020-06-01 00:42:50 |2020-06-01 01:04:33  |1.0            |5.6          |79          |226         |1.0         |19.5       |3.0  |0.5    |2.0       |0.0         |0.3                  |25.3        |2020-06-01 00:42:50|2020-06-01 01:04:33|2020-06-01 00:00:00|0        |2020-06-01 01:00:00|1        |\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+-------------------+-------------------+---------+-------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:46:19 WARN TaskSetManager: Stage 1 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "df_spark_taxi_trip_data.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba14fb2-16c4-4a33-9a0f-b012f8a89934",
   "metadata": {},
   "source": [
    "## Create a table view of the data frame created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb43510-cf7b-49ce-8809-647757e5a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a temporary view from the dataframe\n",
    "\n",
    "df_spark_taxi_trip_data.createOrReplaceTempView(\"table_spark_taxi_trip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217c559-8eee-4ae5-a77c-22283ae9f12f",
   "metadata": {},
   "source": [
    "### Data preparation and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f06dc-c9ce-44c6-9846-f8babefda9a9",
   "metadata": {},
   "source": [
    "#### Scenario 1 - Cancellation trip data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc58db",
   "metadata": {},
   "source": [
    "#### Observation: \n",
    "We found few trips with same PULocationID, DOLocationID, trip_distance, pick_date. Also, they have same fare amount and total amount but one trip has positive value and other has negative value. Which clearly reflects these are cancellation trips. \n",
    "\n",
    "#### Assumption: \n",
    "We are considering the trips as cancelled if they have same PULocationID, DOLocationID, trip_distance, pick_date and also fare amount and total amounts are same but one trip having them in postive values and other in negative values. We are and removing them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2f10e5-3a33-4c4d-b2a7-3fafcb088c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the cancellation data in dataframe using spark query\n",
    "\n",
    "df_trip_cancellation = spark.sql('''\n",
    "SELECT DISTINCT tbl_a.* FROM \n",
    "table_spark_taxi_trip as tbl_a\n",
    "INNER JOIN table_spark_taxi_trip as tbl_b\n",
    "ON tbl_a.total_amount = tbl_b.total_amount * -1\n",
    "AND tbl_a.PULocationID = tbl_b.PULocationID\n",
    "AND tbl_a.DOLocationID = tbl_b.DOLocationID\n",
    "AND tbl_a.trip_distance = tbl_b.trip_distance\n",
    "AND tbl_a.fare_amount = tbl_b.fare_amount * -1\n",
    "AND tbl_a.pick_date = tbl_b.pick_date\n",
    "WHERE tbl_a.total_amount<>0\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77afb9a5-4a1c-4238-9eca-70533e7b53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_cancellation.createOrReplaceTempView(\"table_spark_cancellation_trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "187d0c5f-c7f0-4800-85e3-581044db3fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:46:21 WARN TaskSetManager: Stage 2 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:46:24 WARN TaskSetManager: Stage 3 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 5:================================================>      (177 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+\n",
      "|total_amount|CC |\n",
      "+------------+---+\n",
      "|59.8        |9  |\n",
      "|-59.8       |9  |\n",
      "|57.3        |6  |\n",
      "|-57.3       |6  |\n",
      "|55.3        |38 |\n",
      "|-55.3       |38 |\n",
      "|-52.8       |24 |\n",
      "|52.8        |24 |\n",
      "|50.8        |3  |\n",
      "|-50.8       |3  |\n",
      "+------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Spark query to check the count of negative total_amount values who have same corresponding positive values and are more than single trips \n",
    "\n",
    "spark.sql('''\n",
    "SELECT total_amount,COUNT(*) AS CC \n",
    "FROM table_spark_cancellation_trip\n",
    "GROUP BY total_amount\n",
    "-- HAVING CC>2\n",
    "ORDER BY ABS(total_amount) DESC\n",
    "''').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091868a8-d617-4f31-af85-e4f105bdfed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:46:33 WARN TaskSetManager: Stage 7 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:46:37 WARN TaskSetManager: Stage 10 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:46:38 WARN TaskSetManager: Stage 11 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 13:================================================>     (180 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Showing the count of all trips vs count of cancellation trips\n",
    "\n",
    "print(df_spark_taxi_trip_data.distinct().count())\n",
    "print(df_trip_cancellation.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de41a5a2-c6c8-4648-a8aa-f1bcb99ca39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substracting the cancelled trips as part of cleaning of data \n",
    "# as the cancelled trips wont contribute to effective data analytics\n",
    "\n",
    "df_spark_clean_cancellation = df_spark_taxi_trip_data.distinct().subtract(df_trip_cancellation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb0632e3-4e88-4678-a4bb-eba8fadf9fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:46:44 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/09/07 19:46:44 WARN TaskSetManager: Stage 15 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:46:46 WARN TaskSetManager: Stage 16 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:46:47 WARN TaskSetManager: Stage 19 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "545343"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_clean_cancellation.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73ffb52a-22f8-42c0-9e64-2746cc03693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_clean_cancellation.createOrReplaceTempView(\"table_spark_clean_cancellation_trip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882828b",
   "metadata": {},
   "source": [
    "#### Scenario 2 - Trip distance and Total amount is 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03012dd-fdcd-44b5-ae4c-1a4dc57d41ee",
   "metadata": {},
   "source": [
    "#### Observation: \n",
    "We found few trips where both trip_distance and total_amount having 0 values. \n",
    "\n",
    "#### Assumption: \n",
    "We are considering the trips didn't happen if both trip_distance and total_amount have 0 values and removing them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e345d4b-1bd2-4426-9032-c0f3ec58f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the no-trips data into a dataframe\n",
    "\n",
    "df_spark_clean_dist_amt = spark.sql('''\n",
    "SELECT * \n",
    "FROM table_spark_clean_cancellation_trip\n",
    "WHERE trip_distance = 0 AND total_amount = 0\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f427b2cd-1a81-424e-b774-e970a4752281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:47:02 WARN TaskSetManager: Stage 26 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:47:03 WARN TaskSetManager: Stage 23 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:47:05 WARN TaskSetManager: Stage 24 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_clean_dist_amt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69f8e0b5-cfe3-4b3e-b772-23e92ec08d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substracting all the no trips data as part of cleaning of data as these trip data dont add value to the analysis\n",
    "\n",
    "df_spark_clean_cancellation_dist_amt = df_spark_clean_cancellation.distinct().subtract(df_spark_clean_dist_amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38801df6-a8bd-48f8-b4e2-16085dd58ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:47:15 WARN TaskSetManager: Stage 32 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:47:16 WARN TaskSetManager: Stage 31 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:47:16 WARN TaskSetManager: Stage 33 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:47:17 WARN TaskSetManager: Stage 37 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "545151"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_clean_cancellation_dist_amt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1843699-a24f-4ead-b63b-2359f2fb3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_clean_cancellation_dist_amt.createOrReplaceTempView(\"table_spark_clean_dist_amt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_distance and total_amount having 0 values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9dea34",
   "metadata": {},
   "source": [
    "#### Scenario 3 - Trip distance is greater than 10,000 miles and corresponding Total amount is very less in comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea11299-6e95-4514-bb15-1c29e4ce7d0e",
   "metadata": {},
   "source": [
    "#### Observation: \n",
    "We found few trips where trip_distance is greater than 10,000 miles but its corresponding fare_amount and total_amount is very less and seems incorrect entries.\n",
    "\n",
    "#### Assumption: \n",
    "We are considering the trips as outliers and removing them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52c7e8ec-868d-4a22-9840-c4e64ec7f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the trip records where trip_distance < 10000 as valid records\n",
    "\n",
    "df_spark_clean_trip_dist_outliers = spark.sql('''\n",
    "SELECT * \n",
    "FROM table_spark_clean_dist_amt\n",
    "WHERE trip_distance < 10000\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5e594d2-0fac-4377-a647-10601d96fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:47:38 WARN TaskSetManager: Stage 43 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:47:39 WARN TaskSetManager: Stage 42 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:47:39 WARN TaskSetManager: Stage 44 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:47:40 WARN TaskSetManager: Stage 48 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "545146"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_clean_trip_dist_outliers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61a39948-b7b9-4ba2-b59d-9737d459b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_clean_trip_dist_outliers.createOrReplaceTempView(\"table_spark_clean_outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a676c78",
   "metadata": {},
   "source": [
    "#### Scenario 4 - Trip distance is 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a038c7b-5c4b-497d-b23c-67d3905db314",
   "metadata": {},
   "source": [
    "#### Observation: \n",
    "We found few trips where trip_distance is 0 but its corresponding fare_amount and total_amount is of non-zero value which seems not feasible scenario and seems incorrect entries.\n",
    "\n",
    "#### Assumption: \n",
    "We are considering the trips as invalid and outliers and removing them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4e7cfb4-cfb2-4844-911b-ed19d53e76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the trip records as valid records where trip_distance is non-zero \n",
    "\n",
    "df_spark_clean_valid_trip = spark.sql('''\n",
    "SELECT *\n",
    "FROM table_spark_clean_outliers\n",
    "WHERE trip_distance <> 0\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97ecd212-6f77-4aa9-860f-bd23fa8c3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_clean_valid_trip.createOrReplaceTempView(\"table_spark_clean_valid_trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d577ffb6-77d6-494d-8272-e6cdef2c5cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 19:47:59 WARN TaskSetManager: Stage 53 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:48:00 WARN TaskSetManager: Stage 54 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:48:01 WARN TaskSetManager: Stage 56 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 19:48:02 WARN TaskSetManager: Stage 60 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "528419"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_clean_valid_trip.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81e93f",
   "metadata": {},
   "source": [
    "#### Scenario 5 - Null values for Passenger count and Payment type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83efcd",
   "metadata": {},
   "source": [
    "#### Observation: \n",
    "We found few trips where both passenger_count and payment_type have null values.\n",
    "\n",
    "#### Assumption: \n",
    "We imputed these data by replacing the null values in the passenger_count with 0 and replacing the null values in the payment_type with 5 which indicates unknown payment type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73d7ee10-726e-4462-b088-281582db50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark query to replace null values in the passenger_count with 0 and replacing the null values in the payment_type with 5\n",
    "\n",
    "df_spark_clean = spark.sql('''\n",
    "SELECT \n",
    "tpep_pickup_datetime,\n",
    "tpep_dropoff_datetime,\n",
    "COALESCE(CAST(passenger_count AS int),0) AS passenger_count,\n",
    "trip_distance,\n",
    "PULocationID,\n",
    "DOLocationID,\n",
    "COALESCE(CAST(payment_type AS int),5) AS payment_type,\n",
    "fare_amount,\n",
    "extra,\n",
    "mta_tax,\n",
    "tip_amount,\n",
    "tolls_amount,\n",
    "improvement_surcharge,\n",
    "total_amount,\n",
    "pick_date,\n",
    "pick_date_hour,\n",
    "pick_hour,\n",
    "drop_date,\n",
    "drop_date_hour,\n",
    "drop_hour\n",
    "FROM table_spark_clean_valid_trip\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59f34836-2e1a-4266-9625-00b393c676cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 20:01:08 WARN TaskSetManager: Stage 120 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:01:09 WARN TaskSetManager: Stage 121 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:01:10 WARN TaskSetManager: Stage 123 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:01:10 WARN TaskSetManager: Stage 126 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/home/hadoopuser/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:175: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/hadoopuser/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:175: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/hadoopuser/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:175: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/hadoopuser/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:175: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/hadoopuser/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:175: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/hadoopuser/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:175: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "df_spark_clean_pd = df_spark_clean.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3b24b05-6726-4b1f-af84-173e838308bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tpep_pickup_datetime     0\n",
       "tpep_dropoff_datetime    0\n",
       "passenger_count          0\n",
       "trip_distance            0\n",
       "PULocationID             0\n",
       "DOLocationID             0\n",
       "payment_type             0\n",
       "fare_amount              0\n",
       "extra                    0\n",
       "mta_tax                  0\n",
       "tip_amount               0\n",
       "tolls_amount             0\n",
       "improvement_surcharge    0\n",
       "total_amount             0\n",
       "pick_date                0\n",
       "pick_date_hour           0\n",
       "pick_hour                0\n",
       "drop_date                0\n",
       "drop_date_hour           0\n",
       "drop_hour                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying if there exists any null values in the data\n",
    "\n",
    "df_spark_clean_pd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e101bc98-aecd-41b4-b755-d568d6d4fdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 20:02:28 WARN TaskSetManager: Stage 130 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:02:29 WARN TaskSetManager: Stage 131 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:02:30 WARN TaskSetManager: Stage 134 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:02:31 WARN TaskSetManager: Stage 137 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 138:==================================================>  (190 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+-------------------+---------+-------------------+-------------------+---------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|pick_date          |pick_date_hour     |pick_hour|drop_date          |drop_date_hour     |drop_hour|\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+-------------------+---------+-------------------+-------------------+---------+\n",
      "|2020-06-04 16:01:00 |2020-06-04 16:41:00  |0              |7.53         |65          |39          |0           |20.39      |0.0  |0.5    |2.75      |0.0         |0.3                  |23.94       |2020-06-04 16:01:00|2020-06-04 16:00:00|16       |2020-06-04 16:41:00|2020-06-04 16:00:00|16       |\n",
      "|2020-06-05 12:31:36 |2020-06-05 12:50:16  |1              |7.3          |263         |127         |2           |23.0       |2.5  |0.5    |0.0       |0.0         |0.3                  |26.3        |2020-06-05 12:31:36|2020-06-05 12:00:00|12       |2020-06-05 12:50:16|2020-06-05 12:00:00|12       |\n",
      "|2020-06-07 11:13:20 |2020-06-07 11:17:19  |1              |1.2          |24          |166         |2           |5.5        |0.0  |0.5    |0.0       |0.0         |0.3                  |6.3         |2020-06-07 11:13:20|2020-06-07 11:00:00|11       |2020-06-07 11:17:19|2020-06-07 11:00:00|11       |\n",
      "|2020-06-07 11:23:24 |2020-06-07 11:27:05  |1              |1.0          |236         |141         |1           |5.5        |2.5  |0.5    |1.75      |0.0         |0.3                  |10.55       |2020-06-07 11:23:24|2020-06-07 11:00:00|11       |2020-06-07 11:27:05|2020-06-07 11:00:00|11       |\n",
      "|2020-06-07 12:26:48 |2020-06-07 12:33:39  |1              |1.41         |164         |68          |1           |7.5        |0.0  |0.5    |1.0       |0.0         |0.3                  |11.8        |2020-06-07 12:26:48|2020-06-07 12:00:00|12       |2020-06-07 12:33:39|2020-06-07 12:00:00|12       |\n",
      "+--------------------+---------------------+---------------+-------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-------------------+-------------------+---------+-------------------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark_clean.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c7a1c",
   "metadata": {},
   "source": [
    "#### Storing the clean data in a spark temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58b58ff1-dbcc-45be-9099-b5a6c229e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_clean.createOrReplaceTempView(\"table_spark_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e55952-ef05-495c-8637-d2df4cba11ef",
   "metadata": {},
   "source": [
    "### 2. Count the number of taxi trips for each hour "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c3763-0502-476a-b4d3-c4c241023308",
   "metadata": {},
   "source": [
    "Running query in dataframe operations as we are asked to write Spark programs either in pySpark or Scala to do the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572653e",
   "metadata": {},
   "source": [
    "#### Assumption:\n",
    "In the question it is mentioned to count the number of taxi trips for each hour.<br> \n",
    "We are assuming - <br> \n",
    "1. The hour as 24 hours of the day.<br>\n",
    "2. Also, a trip gets started from the pickup hour so we are considering the column pick_hour for our calculations for this question.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41afb899-0fbd-4e7d-8af1-6463204733d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_clean_trip_hour = df_spark_clean.groupby('pick_hour').count().select('pick_hour', col('count').alias('count_taxi_trip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f3754df-d4cd-423f-948b-b18038f4d61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 20:03:02 WARN TaskSetManager: Stage 140 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:03:02 WARN TaskSetManager: Stage 141 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:03:03 WARN TaskSetManager: Stage 143 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:03:05 WARN TaskSetManager: Stage 147 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 149:===============================================>     (180 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|pick_hour|count_taxi_trip|\n",
      "+---------+---------------+\n",
      "|0        |7633           |\n",
      "|1        |6146           |\n",
      "|2        |4797           |\n",
      "|3        |4791           |\n",
      "|4        |6617           |\n",
      "|5        |6684           |\n",
      "|6        |14548          |\n",
      "|7        |19454          |\n",
      "|8        |24073          |\n",
      "|9        |27546          |\n",
      "|10       |30789          |\n",
      "|11       |33632          |\n",
      "|12       |36255          |\n",
      "|13       |37648          |\n",
      "|14       |38820          |\n",
      "|15       |39306          |\n",
      "|16       |37237          |\n",
      "|17       |37034          |\n",
      "|18       |33179          |\n",
      "|19       |25610          |\n",
      "|20       |17866          |\n",
      "|21       |14427          |\n",
      "|22       |12749          |\n",
      "|23       |11578          |\n",
      "+---------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark_clean_trip_hour.sort(asc('pick_hour')).show(25,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab185cd-5db2-4f60-8d99-c0ede972b029",
   "metadata": {},
   "source": [
    "### 3. Average fare amount collected by hour of the day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e04f6",
   "metadata": {},
   "source": [
    "#### Assumption:\n",
    "In the question it is mentioned to calculate average fare amount collected by hour of the day.<br>\n",
    "We are assuming - <br> \n",
    "1. The hour of the day as 24 hours of the day.<br>\n",
    "2. Also, a trip gets completed at the dropoff hour so we are considering the column drop_hour for our calculations for this question.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c705c735-edcb-468a-9558-2227a91f1bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 20:05:59 WARN TaskSetManager: Stage 151 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:06:00 WARN TaskSetManager: Stage 152 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:06:01 WARN TaskSetManager: Stage 154 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:06:01 WARN TaskSetManager: Stage 157 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 160:=================================================>   (188 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|drop_hour|avg_fare_amount|\n",
      "+---------+---------------+\n",
      "|0        |19.342         |\n",
      "|1        |25.488         |\n",
      "|2        |30.32          |\n",
      "|3        |34.45          |\n",
      "|4        |38.999         |\n",
      "|5        |29.448         |\n",
      "|6        |12.455         |\n",
      "|7        |10.927         |\n",
      "|8        |11.031         |\n",
      "|9        |10.824         |\n",
      "|10       |11.171         |\n",
      "|11       |11.535         |\n",
      "|12       |11.52          |\n",
      "|13       |11.488         |\n",
      "|14       |11.422         |\n",
      "|15       |12.078         |\n",
      "|16       |12.952         |\n",
      "|17       |13.081         |\n",
      "|18       |13.24          |\n",
      "|19       |12.794         |\n",
      "|20       |13.299         |\n",
      "|21       |15.157         |\n",
      "|22       |15.846         |\n",
      "|23       |17.913         |\n",
      "+---------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT\n",
    "  drop_hour,\n",
    "  ROUND(AVG(fare_amount),3) AS avg_fare_amount\n",
    "FROM table_spark_clean\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "''').show(25, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78f04d-0dbd-4317-a7d7-a2b0e283c102",
   "metadata": {},
   "source": [
    "### 4. Average fare amount compared to the average trip distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2413d941",
   "metadata": {},
   "source": [
    "#### Assumption:\n",
    "In the question it is mentioned to calculate Average fare amount compared to the average trip distance.<br>\n",
    "We are assuming - <br> \n",
    "1. Showing the comparision with respect to the dropoff hour of the day.<br>\n",
    "2. The hour as 24 hours of the day.<br>\n",
    "3. Also, a trip gets completed at the dropoff hour so we are considering the column drop_hour for our calculations for this question.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "61bf7466-6902-48bd-9b86-082009513de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 20:21:16 WARN TaskSetManager: Stage 195 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:21:17 WARN TaskSetManager: Stage 196 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:21:18 WARN TaskSetManager: Stage 199 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:21:18 WARN TaskSetManager: Stage 201 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 204:================================================>    (182 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|avg_fare_amount|avg_trip_distance|\n",
      "+---------------+-----------------+\n",
      "|19.342         |5.999            |\n",
      "|25.488         |7.128            |\n",
      "|30.32          |8.152            |\n",
      "|34.45          |9.316            |\n",
      "|38.999         |10.665           |\n",
      "|29.448         |8.931            |\n",
      "|12.455         |3.519            |\n",
      "|10.927         |2.756            |\n",
      "|11.031         |2.677            |\n",
      "|10.824         |2.525            |\n",
      "|11.171         |2.653            |\n",
      "|11.535         |2.788            |\n",
      "|11.52          |2.78             |\n",
      "|11.488         |2.762            |\n",
      "|11.422         |2.709            |\n",
      "|12.078         |2.914            |\n",
      "|12.952         |3.206            |\n",
      "|13.081         |3.278            |\n",
      "|13.24          |3.453            |\n",
      "|12.794         |3.383            |\n",
      "|13.299         |3.614            |\n",
      "|15.157         |4.296            |\n",
      "|15.846         |4.571            |\n",
      "|17.913         |5.442            |\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT drop_hour,\n",
    "  ROUND(AVG(fare_amount),3) AS avg_fare_amount,\n",
    "  ROUND(AVG(trip_distance),3) AS avg_trip_distance\n",
    "FROM\n",
    "  table_spark_clean\n",
    "GROUP BY\n",
    "  drop_hour\n",
    "ORDER BY\n",
    "  drop_hour;\n",
    "''').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e4337-58d0-4965-b9b5-675446fb32eb",
   "metadata": {},
   "source": [
    "### 5. Average fare amount and average trip distance by day of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943487a",
   "metadata": {},
   "source": [
    "#### Assumption:\n",
    "In the question it is mentioned to calculate average fare amount and average trip distance by day of the week.<br>\n",
    "We are assuming - <br> \n",
    "1. The day of the week as 7 days of the week starting from Sunday (Day-1).<br>\n",
    "2. Also, fare is paid on a trip completion which gets completed at the dropoff hour for the date so we are considering the column drop_date for our calculations for this question.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05f0ec00-4fe1-49d2-b50c-f34ad3eec8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 20:26:53 WARN TaskSetManager: Stage 207 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:26:54 WARN TaskSetManager: Stage 206 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:26:55 WARN TaskSetManager: Stage 208 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 20:26:55 WARN TaskSetManager: Stage 212 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 215:===================================================> (193 + 7) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------------+\n",
      "|day_of_week|avg_fare_amount|avg_trip_distance|\n",
      "+-----------+---------------+-----------------+\n",
      "|1          |14.524         |4.051            |\n",
      "|2          |13.288         |3.385            |\n",
      "|3          |13.284         |3.319            |\n",
      "|4          |13.185         |3.342            |\n",
      "|5          |13.327         |3.395            |\n",
      "|6          |13.553         |3.448            |\n",
      "|7          |13.87          |3.735            |\n",
      "+-----------+---------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT  \n",
    "  CASE WHEN DAYOFWEEK(drop_date) = 1 THEN 'Sunday'\n",
    "  CASE WHEN DAYOFWEEK(drop_date) = 2 THEN 'Monday' \n",
    "  CASE WHEN DAYOFWEEK(drop_date) = 3 THEN 'Tuesday'\n",
    "  CASE WHEN DAYOFWEEK(drop_date) = 4 THEN 'Wednesday'\n",
    "  CASE WHEN DAYOFWEEK(drop_date) = 5 THEN 'Thursday'\n",
    "  CASE WHEN DAYOFWEEK(drop_date) = 6 THEN 'Friday'\n",
    "  CASE WHEN DAYOFWEEK(drop_date) = 7 THEN 'Saturday'\n",
    "  END AS day_of_week,\n",
    "  ROUND(AVG(fare_amount),3) AS avg_fare_amount,\n",
    "  ROUND(AVG(trip_distance),3) AS avg_trip_distance\n",
    "FROM\n",
    "  table_spark_clean\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "''').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae32ff-6ef4-4dc6-be25-23ec8152dc9b",
   "metadata": {},
   "source": [
    "### 6. In the month of June 2020, find the zone which had maximum number of pick ups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888191c5",
   "metadata": {},
   "source": [
    "##### Reading the taxi+_zone_lookup.csv file from hdfs using pandas read function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08242669-a155-46e7-9b16-3c486b497ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the taxi+_zone_lookup.csv file from hdfs using pandas read function\n",
    "\n",
    "df_taxi_lookup = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"hdfs://localhost:9000/BDS_G106/taxi+_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "872fbbbb-5c26-40f8-96cd-d56ca82a67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi_lookup.createOrReplaceTempView(\"table_taxi_lookup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "df0f3561-f7f9-404c-bf4a-38679990eeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 21:01:09 WARN TaskSetManager: Stage 273 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:01:10 WARN TaskSetManager: Stage 274 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:01:10 WARN TaskSetManager: Stage 277 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:01:12 WARN TaskSetManager: Stage 279 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 282:====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+\n",
      "|Zone                 |pickup_count|\n",
      "+---------------------+------------+\n",
      "|Upper East Side North|22658       |\n",
      "+---------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Joining both taxi_zone_lookup and yellow_tripdata data to get the zone details on basis of locationids \n",
    "# to get maximum number of pick ups\n",
    "\n",
    "spark.sql('''SELECT\n",
    "  tbl_lookup.Zone,\n",
    "  COUNT(tbl_clean.PULocationID) AS pickup_count\n",
    "FROM\n",
    "  table_spark_clean AS tbl_clean\n",
    "  INNER JOIN table_taxi_lookup AS tbl_lookup\n",
    "  ON tbl_clean.PULocationID =  CAST(tbl_lookup.LocationID AS bigint)\n",
    "WHERE\n",
    "  YEAR(tbl_clean.pick_date) = 2020\n",
    "  AND MONTH(tbl_clean.pick_date) = 6\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 1;\n",
    "''').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c256cd7-f4c3-4568-a441-15a04d8e38bf",
   "metadata": {},
   "source": [
    "### 7. In the month of June 2020, find the zone which had maximum number of drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b29df9e8-541d-4f70-af4a-3bf7fe592652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 21:13:45 WARN TaskSetManager: Stage 286 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:13:47 WARN TaskSetManager: Stage 287 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:13:48 WARN TaskSetManager: Stage 290 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:13:50 WARN TaskSetManager: Stage 292 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+\n",
      "|Zone                 |drop_count|\n",
      "+---------------------+----------+\n",
      "|Upper East Side North|21836     |\n",
      "+---------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining both taxi_zone_lookup and yellow_tripdata data to get the zone details on basis of locationids \n",
    "# to get maximum number of drops\n",
    "\n",
    "\n",
    "spark.sql('''SELECT\n",
    "  tbl_lookup.Zone,\n",
    "  COUNT(tbl_clean.DOLocationID) AS drop_count\n",
    "FROM\n",
    "  table_spark_clean AS tbl_clean\n",
    "  INNER JOIN table_taxi_lookup AS tbl_lookup\n",
    "  ON tbl_clean.DOLocationID =  CAST(tbl_lookup.LocationID AS bigint)\n",
    "WHERE\n",
    "  YEAR(tbl_clean.drop_date) = 2020\n",
    "  AND MONTH(tbl_clean.drop_date) = 6\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 1;\n",
    "''').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a093d011-921e-4d15-bae7-56dbe87697cf",
   "metadata": {},
   "source": [
    "### 8. Average no of passengers by hour of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e44861",
   "metadata": {},
   "source": [
    "#### Assumption:\n",
    "In the question it is mentioned to count the average no of passengers by hour of the day<br> \n",
    "We are assuming - <br> \n",
    "1. The hour as 24 hours of the day.<br>\n",
    "2. Also, a trip gets started from the pickup hour so we are considering the column pick_hour for our calculations for this question.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d5ee6fd-4b3a-4e32-acb3-58f92178e558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 21:31:59 WARN TaskSetManager: Stage 331 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:31:59 WARN TaskSetManager: Stage 332 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:32:00 WARN TaskSetManager: Stage 335 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:32:02 WARN TaskSetManager: Stage 337 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 340:==============================================>      (177 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|pick_hour|avg_passenger_count|\n",
      "+---------+-------------------+\n",
      "|0        |1                  |\n",
      "|1        |1                  |\n",
      "|2        |1                  |\n",
      "|3        |1                  |\n",
      "|4        |1                  |\n",
      "|5        |1                  |\n",
      "|6        |2                  |\n",
      "|7        |2                  |\n",
      "|8        |2                  |\n",
      "|9        |2                  |\n",
      "|10       |2                  |\n",
      "|11       |2                  |\n",
      "|12       |2                  |\n",
      "|13       |2                  |\n",
      "|14       |2                  |\n",
      "|15       |2                  |\n",
      "|16       |2                  |\n",
      "|17       |2                  |\n",
      "|18       |2                  |\n",
      "|19       |2                  |\n",
      "|20       |2                  |\n",
      "|21       |2                  |\n",
      "|22       |2                  |\n",
      "|23       |2                  |\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT\n",
    "  pick_hour,\n",
    "  CEIL(ROUND(AVG(CAST(passenger_count AS int)),3)) AS avg_passenger_count -- CEIL (can remove later)\n",
    "FROM\n",
    "  table_spark_clean\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "''').show(25,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0684882-b5e6-48d8-ba81-946d2c634e12",
   "metadata": {},
   "source": [
    "### 9. Total number of payments made by different type for the month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879a3b5",
   "metadata": {},
   "source": [
    "#### Assumption:\n",
    "In the question it is mentioned to count the total number of payments made by different type for the month.<br> \n",
    "We are assuming - <br> \n",
    "1. Showing the comparision with respect to the payment_type and month. So, considering a payment_type is done at different month, it is captured seperately at monthly level.<br>\n",
    "2. The month as 12 months of the year starting from January (Month-1).<br>\n",
    "3. Also, payment is done on a trip completion which gets completed at the dropoff hour for the date so we are considering the column drop_date for our calculations for this question.<br>\n",
    "4. As mentioned in the pre-processing section above we imputed null values in the passenger_count with 0 and the null values in the payment_type with 5 which indicates unknown payment type.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ad0a41bd-3b63-4d33-ab25-d090965bc618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 21:40:27 WARN TaskSetManager: Stage 353 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:40:28 WARN TaskSetManager: Stage 354 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:40:29 WARN TaskSetManager: Stage 357 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/07 21:40:30 WARN TaskSetManager: Stage 360 contains a task of very large size (7723 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 362:===================================================> (195 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+------------+\n",
      "|payment_type|Month|payment_type|\n",
      "+------------+-----+------------+\n",
      "|           0|    6|       49256|\n",
      "|           0|    7|           9|\n",
      "|           1|    6|      310678|\n",
      "|           1|    7|          59|\n",
      "|           2|    1|           3|\n",
      "|           2|    6|      163367|\n",
      "|           2|    7|          36|\n",
      "|           3|    6|        3847|\n",
      "|           4|    6|        1152|\n",
      "|           5|    6|          12|\n",
      "+------------+-----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT\n",
    "  CASE WHEN payment_type = 1 THEN 'Credit card'\n",
    "  CASE WHEN payment_type = 2 THEN 'Cash' \n",
    "  CASE WHEN payment_type = 3 THEN 'No charge'\n",
    "  CASE WHEN payment_type = 4 THEN 'Dispute'\n",
    "  CASE WHEN payment_type = 5 THEN 'Unknown'\n",
    "  CASE WHEN payment_type = 6 THEN 'Voided trip'\n",
    "  END AS payment_type,\n",
    "  MONTH(drop_date) AS Month,\n",
    "  COUNT(payment_type) AS payment_type\n",
    "FROM\n",
    "  table_spark_clean\n",
    "GROUP BY 1,2\n",
    "ORDER BY 1,2\n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
